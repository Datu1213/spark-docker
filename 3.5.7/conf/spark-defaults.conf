# Those two configurations are mandatory if you want to use Delta Lake
# Exceptions will be thrown if you just add the delta-spark package
# without them.
# --------------------------------------------------------------
# Delta Lake extensions

spark.sql.extensions io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog org.apache.spark.sql.delta.catalog.DeltaCatalog



# Specify the location for the Spark SQL warehouse and metastore_db.

# [ Optional ]
#spark.sql.warehouse.dir=/opt/spark/work-dir/warehouse
#spark.sql.metastore_db.dir=/opt/spark/work-dir/metastore_db

# spark.jars.packages will call Apache Ivy to download the specified
# packages and their dependencies to the local Ivy repository.
# The default location is $HOME/.ivy2,
# but $HOME is not set in this containerized Spark environment.
# It will lead to nonexistent directory and java.io.FileNotFoundException.
# ----------------------------------------------------------------
# Ivy local library path

spark.jars.ivy /opt/spark/.ivy2

# Use "\" to split long lines for better readability.
# Mind the versions of packages 
# to be compatible with your Spark and Scala versions!!!!!
# ----------------------------------------------------------------
# packages
# Delta Lake 3.3.0 for Spark 3.5.7 and Scala 2.12
# Elasticsearch-Hadoop connector
# Spark SQL Kafka connector
# AWS Java SDK bundle
# Hadoop AWS module
spark.jars.packages io.delta:delta-spark_2.12:3.3.0,io.delta:delta-storage:3.3.0,org.elasticsearch:elasticsearch-spark-30_2.12:8.11.3,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.7,com.amazonaws:aws-java-sdk-bundle:1.12.534,org.apache.hadoop:hadoop-aws:3.3.4

